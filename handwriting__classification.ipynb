{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "handwriting _classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPgJY4vK7JTKNfl9jMo3NVJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishwarya418112/neural-networks-and-deep-learning/blob/master/handwriting__classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M94SpHes0DoJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class Network(object):\n",
        "  def __init__(self, sizes):\n",
        "    self.num_layers = len(sizes)\n",
        "    self.sizes = sizes\n",
        "    self.biases = [np.random.randn(y, 1) for y in sizes[1:]]#single dimension matrix\n",
        "    self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]# \n",
        "\n",
        "    \n",
        "  def feedforward(self, a):\n",
        "    for b, w in zip(self.bias, self.weights):\n",
        "      a = sigmoid(np.dot(w, a) + b)\n",
        "    return a\n",
        "\n",
        "  def SGD(self, training_data, epochs, mini_batch_size, eta, test_size = None):\n",
        "    if test_data: ntest = len(test_data)\n",
        "    n =len(training_data)\n",
        "    for j in xrange(epochs):\n",
        "      random.shuffle(training_data) # first shuffle the entire training data before creating the mini batches\n",
        "      mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "      for mini_batch in mini_batches:\n",
        "        self.update_mini_batch(mini_batch, eta)\n",
        "      if test_data:\n",
        "        print('Epoch{0}: {1} / {2}'.format(j, self.evaluate(test_data), n_test)) \n",
        "        \"\"\"  \n",
        "        if there is atleast one test data, evaluate the epoch and return the number of correct \n",
        "        test data outputs over the total number of test data points\n",
        "        {} is only a place holder catering to format()\n",
        "        \"\"\" \n",
        "      else:\n",
        "        print('Epoch{0}: Complete'.format(j ))# else continue with training the epoch \n",
        "  \"\"\"\n",
        "  1. The training data is split into mini batches with each epoch after shuffling to randomize the batches in each epoch\n",
        "  2. For each mini batch a single step of gradient descent is applied that updates the weights and biases and hence with each batch \n",
        "     we get a new set of weights and biases\n",
        "  \"\"\"\n",
        "  def update_mini_batch(self, mini_batch, eta):\n",
        "    \"\"\"\n",
        "    update the weights and biases of the neural network by applying GD using backpropagation to a single mini batch\n",
        "    mini batch is a list of tuples (x,y) a subset taken from the training data.\n",
        "    \"\"\"\n",
        "    b = [np.zeros(b.shape) for b in self.biases]\n",
        "    w = [np.zeros(w.shape) for w in self.weights] \n",
        "    \"\"\"\n",
        "    shape is used to create a zero initialized matrix of the same dimensions as the respective \n",
        "    b or w\n",
        "    \"\"\"\n",
        "    for x, y in mini_batch:\n",
        "      delta_b, delta_w = self.backprop(x, y)\n",
        "      b = [b+db for b, db in zip(b, delta_b)]\n",
        "      w = [w+dw for w, dw in zip(w, delta_w)]\n",
        "    self.weights = [w - (eta/len(mini_batch))*new_w for w, new_w in zip(self.weights, w)]\n",
        "    self.biases = [b - (eta/len(mini_batch))*new_b for b, new_b in zip(self.biases, b)]\n",
        "    \"\"\"\n",
        "    1. We take a randomized value of weights and biases to initialize the neurons and synapses\n",
        "    2. Start with zero initialized matrix of weights and biases that will get better at every iteration of the backprop algorithm\n",
        "    3. Now fine tune the previous randomly taken weights and biases to optimize the solution (cost function estimate using backprop)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "net = Network([2, 3, 1])\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1.0/(1.0 + np.exp(-z))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bias = [9, 10, 11]\n",
        "weights = [[1,2], [2,3],[3,4]]\n",
        "print(list(zip(bias[:-1], bias[1:])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqsI1d_c4tOg",
        "outputId": "c1a31df6-8d14-40ac-a3ad-1fbca5e24ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(9, 10), (10, 11)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "q19J2NpaL8wR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}